{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02** DistillBERT 파인튜닝 및 평가\n",
    "\n",
    "트랜스포머 기반의 사전학습 모델은 토큰과 문장들의 관계를 레이블이 없는 대규모 텍스트를 가지고 지도학습을 이미 완료했습니다. 이에 레이블이 부여된 소규모 데이터를 대상으로 파인튜닝을 하면 이 모델은 높은 정확도의 예측을 할 수 있습니다. 이를 위해 허깅페이스의 `Trainer` 클래스와 통상적인 파이토치 학습 방법을 사용합니다.\n",
    "\n",
    "### 사전 준비: GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **004** IMDB 데이터세트\n",
    "\n",
    "영화 리뷰 코멘트의 긍정적/부정적 감성을 판단하기 위해 사용하는 데이터세트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split=\"train\")\n",
    "test_iter = IMDB(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, \"I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\")\n",
      "(1, 'This was an abysmal show. In short it was about this kid called Doug who guilt-tripped a lot. Seriously he could feel guilty over killing a fly then feeling guilty over feeling guilty for killing the fly and so forth. The animation was grating and unpleasant and the jokes cheap. <br /><br />It aired here in Sweden as a part of the \"Disney time\" show and i remember liking it some what but then i turned 13.<br /><br />I never got why some of the characters were green and purple too. What was up with that? <br /><br />Truly a horrible show. Appareantly it spawned a movie which i\\'ve never seen but i don\\'t have any great expectations for that one either.')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(6)\n",
    "\n",
    "train_lists = list(train_iter)\n",
    "test_lists = list(test_iter)\n",
    "\n",
    "train_lists_small = random.sample(train_lists, 1000)\n",
    "test_lists_small = random.sample(test_lists, 1000)\n",
    "\n",
    "print(train_lists_small[0])\n",
    "print(test_lists_small[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **005** 레이블 인코딩\n",
    "\n",
    "데이터세트의 레이블은 긍정인 경우 2, 부정인 경우 1로 부여되어 있습니다. 여기서는 긍정을 의미하는 레이블을 1로, 부정을 의미하는 레이블을 0으로 바꾸겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\n",
      "1\n",
      "This was an abysmal show. In short it was about this kid called Doug who guilt-tripped a lot. Seriously he could feel guilty over killing a fly then feeling guilty over feeling guilty for killing the fly and so forth. The animation was grating and unpleasant and the jokes cheap. <br /><br />It aired here in Sweden as a part of the \"Disney time\" show and i remember liking it some what but then i turned 13.<br /><br />I never got why some of the characters were green and purple too. What was up with that? <br /><br />Truly a horrible show. Appareantly it spawned a movie which i've never seen but i don't have any great expectations for that one either.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "for label, text in train_lists_small:\n",
    "    train_labels.append(1 if label==2 else 0)\n",
    "    train_texts.append(text)\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for label, text in test_lists_small:\n",
    "    test_labels.append(1 if label==2 else 0)\n",
    "    test_texts.append(text)\n",
    "\n",
    "\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "print(test_texts[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **006** 학습 및 검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=3\n",
    ")\n",
    "\n",
    "print(len(train_texts))\n",
    "print(len(val_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **007** 토크나이징 및 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5179ce30bd6b48e192793c5dd951aa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kang MinJae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kang MinJae\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e4533be159483cab9713f1b0ad9cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642d74e660854814a2f183f80902daa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3232d833dfa3454aa71c48e2869736cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4937, 11350, 2038, 2048, 1000, 7592, 14433, 1000, 1011]\n",
      "[CLS] cat soup has two \" hello kitty \" -\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "print(train_encodings[\"input_ids\"][0][:10])\n",
    "print(tokenizer.decode(train_encodings[\"input_ids\"][0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **008** 데이터세트 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDBDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDBDataset(test_encodings, test_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  4937, 11350,  2038,  2048,  1000,  7592, 14433,  1000,  1011,\n",
      "         2828, 18401,  2015, 28866,  2075,  2006,  1037, 13576,  4440,  2083,\n",
      "         1996, 25115,  1010,  2073,  2505,  2064,  4148,  1010,  1998,  2515,\n",
      "         1012,  2023,  2568,  1011,  4440,  4691,  4004,  2460,  3594,  2053,\n",
      "        13764,  8649,  1010,  4942, 21532,  2773, 22163,  2612,  1012,  2045,\n",
      "         2003,  2053,  2126,  1997,  7851,  2023, 17183, 14088,  9476,  3272,\n",
      "         2000,  2425,  2017,  2000,  2156,  2009,  2005,  4426,  1012,  1998,\n",
      "         2191,  2469,  2053,  2028,  2104,  2184,  2003,  1999,  1996,  2282,\n",
      "         1012,  4487,  6491,  6633,  5677,  3672,  1998,  2064,  3490, 10264,\n",
      "         2964,  1998, 18186,  1998,  9576,  2854,  1998,  5573,  2331,  1998,\n",
      "         2655,  3560, 27770,  2005,  2500,  2024,  2691,  6991,  1012,  7481,\n",
      "         1012,  3383,  1996,  2087, 13432,  3746,  2003,  2008,  1997,  2019,\n",
      "        10777,  3605,  1997,  2300,  2008,  1996,  8934,  7368,  9880,  2083,\n",
      "         1998,  1999,  1010,  1998,  2036,  4536,  1012,  2021,  2066,  8134,\n",
      "         2673,  2842,  1999,  2023,  2143,  1010,  2008, 10021,  1010, 27263,\n",
      "        17933,  4226, 25347,  2574,  3310,  2000,  1037,  9202,  2203,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **009** 사전학습 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c2d578996140a7b036317f8d9e5696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **010** TrainingArguments 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"D:/_MODEL_CHECKPOINT/distilbert-base-uncased\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"D:/_MODEL_CHECKPOINT/distilbert-base-uncased/logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **011** GPU로 전송"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **012** Trainer 클래스 사전학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\", \"I have not figured out what the chosen title has to do with the movie.\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "outputs = model(torch.tensor(input_tokens[\"input_ids\"]).to(device))\n",
    "\n",
    "label_dict = {0: \"positive\", 1: \"negative\"}\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs[\"logits\"], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kang MinJae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkminjae618\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Kang MinJae\\Desktop\\Code\\transformers_101\\wandb\\run-20230824_030656-rv2n9gby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kminjae618/huggingface/runs/rv2n9gby' target=\"_blank\">light-violet-49</a></strong> to <a href='https://wandb.ai/kminjae618/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kminjae618/huggingface' target=\"_blank\">https://wandb.ai/kminjae618/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kminjae618/huggingface/runs/rv2n9gby' target=\"_blank\">https://wandb.ai/kminjae618/huggingface/runs/rv2n9gby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b17d79e726485e819e08774da51561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6992, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6977, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6843, 'learning_rate': 3e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6851, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6905, 'learning_rate': 5e-06, 'epoch': 1.0}\n",
      "{'loss': 0.6794, 'learning_rate': 6e-06, 'epoch': 1.2}\n",
      "{'loss': 0.6756, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.4}\n",
      "{'loss': 0.6672, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}\n",
      "{'loss': 0.6289, 'learning_rate': 9e-06, 'epoch': 1.8}\n",
      "{'loss': 0.6114, 'learning_rate': 1e-05, 'epoch': 2.0}\n",
      "{'loss': 0.5085, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4384, 'learning_rate': 1.2e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3489, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.6}\n",
      "{'loss': 0.3817, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.8}\n",
      "{'loss': 0.2754, 'learning_rate': 1.5e-05, 'epoch': 3.0}\n",
      "{'loss': 0.1873, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}\n",
      "{'loss': 0.2103, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.4}\n",
      "{'loss': 0.2352, 'learning_rate': 1.8e-05, 'epoch': 3.6}\n",
      "{'loss': 0.195, 'learning_rate': 1.9e-05, 'epoch': 3.8}\n",
      "{'loss': 0.2059, 'learning_rate': 2e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0873, 'learning_rate': 2.1e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0713, 'learning_rate': 2.2000000000000003e-05, 'epoch': 4.4}\n",
      "{'loss': 0.1397, 'learning_rate': 2.3000000000000003e-05, 'epoch': 4.6}\n",
      "{'loss': 0.0835, 'learning_rate': 2.4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.1334, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n",
      "{'loss': 0.0433, 'learning_rate': 2.6000000000000002e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0125, 'learning_rate': 2.7000000000000002e-05, 'epoch': 5.4}\n",
      "{'loss': 0.0591, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}\n",
      "{'loss': 0.1409, 'learning_rate': 2.9e-05, 'epoch': 5.8}\n",
      "{'loss': 0.1955, 'learning_rate': 3e-05, 'epoch': 6.0}\n",
      "{'loss': 0.0265, 'learning_rate': 3.1e-05, 'epoch': 6.2}\n",
      "{'loss': 0.079, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0435, 'learning_rate': 3.3e-05, 'epoch': 6.6}\n",
      "{'loss': 0.1099, 'learning_rate': 3.4000000000000007e-05, 'epoch': 6.8}\n",
      "{'loss': 0.0143, 'learning_rate': 3.5e-05, 'epoch': 7.0}\n",
      "{'loss': 0.0184, 'learning_rate': 3.6e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0993, 'learning_rate': 3.7e-05, 'epoch': 7.4}\n",
      "{'loss': 0.1309, 'learning_rate': 3.8e-05, 'epoch': 7.6}\n",
      "{'loss': 0.0667, 'learning_rate': 3.9000000000000006e-05, 'epoch': 7.8}\n",
      "{'loss': 0.104, 'learning_rate': 4e-05, 'epoch': 8.0}\n",
      "{'train_runtime': 521.0175, 'train_samples_per_second': 12.284, 'train_steps_per_second': 0.768, 'train_loss': 0.28413260404020546, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.28413260404020546, metrics={'train_runtime': 521.0175, 'train_samples_per_second': 12.284, 'train_steps_per_second': 0.768, 'train_loss': 0.28413260404020546, 'epoch': 8.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\", \"I have not figured out what the chosen title has to do with the movie.\"], truncation=True, padding=True)\n",
    "\n",
    "outputs = model(torch.tensor(input_tokens[\"input_ids\"]).to(device))\n",
    "\n",
    "label_dict = {1: \"positive\", 0: \"negative\"}\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs[\"logits\"], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **013** 파이토치 사전학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, tokenizer):\n",
    "    input_tokens = tokenizer([\"I feel fantastic\", \"My life is going something wrong\", \"I have not figured out what the chosen title has to do with the movie.\"], truncation=True, padding=True)\n",
    "\n",
    "    outputs = model(torch.tensor(input_tokens[\"input_ids\"]).to(device))\n",
    "\n",
    "    label_dict = {1: \"positive\", 0: \"negative\"}\n",
    "\n",
    "    print([label_dict[i] for i in torch.argmax(outputs[\"logits\"], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Kang MinJae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'negative']\n",
      "None\n",
      "epoch: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.to(device)\n",
    "\n",
    "print(test_inference(model, tokenizer))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 최적화 함수 정의\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 모델을 학습 모드로 전환\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(8):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    for batch in train_loader:\n",
    "        # 최적화 함수의 그레디언트 초기화\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # 모델을 사용한 추론\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = outputs[0]\n",
    "        losses.append(loss)\n",
    "\n",
    "        # 오차 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트\n",
    "        optim.step()\n",
    "\n",
    "# 모델을 eval 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "print(test_inference(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
